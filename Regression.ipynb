{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP92OaEHh2la/SvuilNqLMg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5Uxp6Xl-8ke"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Assignment"],"metadata":{"id":"bRuU3nZ5_Lgi"}},{"cell_type":"markdown","source":["1. What is Simple Linear Regression?\n","\n","ans- Simple Linear Regression is a statistical method used to model the relationship between two variables: an independent variable (predictor) and a dependent variable (response). It assumes a linear relationship and aims to find the best-fitting straight line that describes how the dependent variable changes with respect to the independent variable.\n","\n","2. What are the key assumptions of Simple Linear Regression?\n","\n","ans- Linearity: The relationship between the independent and dependent variables is linear.\n","Independence: The errors (residuals) are independent of each other.\n","Homoscedasticity: The variance of the errors is constant across all levels of the independent variable.\n","Normality: The errors are normally distributed.\n","\n","3. What does the coefficient m represent in the equation Y=mX+c?\n","\n","ans- 'm' represents the slope of the regression line. It indicates the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n","\n","4. What does the intercept c represent in the equation Y=mX+c?\n","\n","ans- 'c' represents the y-intercept. It is the value of the dependent variable (Y) when the independent variable (X) is zero.\n","\n","5. How do we calculate the slope m in Simple Linear Regression?\n","\n","ans- The slope 'm' is typically calculated using the least squares method, which minimizes the sum of the squared differences between the observed and predicted values of the dependent variable. The formula involves the covariance between X and Y and the variance of X.\n","\n","6. What is the purpose of the least squares method in Simple Linear Regression?\n","\n","ans- The least squares method is used to find the best-fitting regression line by minimizing the sum of the squared residuals (the differences between the observed and predicted values). This ensures that the line is as close as possible to all data points.\n","\n","7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n","\n","ans- R² represents the proportion of the variance in the dependent variable that is predictable from the independent variable. It ranges from 0 to 1. A higher R² indicates a better fit of the model to the data.\n","\n","8. What is Multiple Linear Regression?\n","\n","ans- Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between a dependent variable and two or more independent variables. It aims to find the best-fitting hyperplane that describes how the dependent variable changes with respect to the independent variables.\n","\n","9. What is the main difference between Simple and Multiple Linear Regression?\n","\n","ans- Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves two or more independent variables.\n","\n","10. What are the key assumptions of Multiple Linear Regression?\n","\n","ans- Multiple Linear Regression assumes linearity, independence, homoscedasticity, and normality of errors. Additionally, it assumes no perfect multicollinearity among the independent variables.\n","\n","11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n","\n","ans- Heteroscedasticity is the condition where the variance of the errors is not constant across all levels of the independent variables. It can lead to unreliable estimates of the regression coefficients and incorrect conclusions about the statistical significance of the model.\n","\n","12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n","\n","ans- Multicollinearity can be addressed by:\n","Removing one of the highly correlated variables.\n","Using dimensionality reduction techniques like Principal Component Analysis (PCA).\n","Increasing the sample size.\n","Using regularization techniques like Ridge or Lasso regression.\n","\n","13. What are some common techniques for transforming categorical variables for use in regression models?\n","\n","ans- Common techniques include:\n","One-Hot Encoding: Creates binary columns for each category.\n","Label Encoding: Assigns a unique integer to each category.\n","Dummy Variables: Similar to one-hot encoding.\n","\n","14. What is the role of interaction terms in Multiple Linear Regression?\n","\n","ans- Interaction terms allow us to model how the effect of one independent variable on the dependent variable changes depending on the value of another independent variable. They capture synergistic or antagonistic effects.\n","\n","15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n","\n","ans- In Simple Linear Regression, the intercept is the value of Y when X is zero. In Multiple Linear Regression, the intercept is the value of Y when all independent variables are zero. The interpretation can be more complex and sometimes less meaningful in Multiple Linear Regression.\n","\n","16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n","\n","ans- The slope indicates the change in the dependent variable for a one-unit change in the independent variable. It affects predictions by determining the magnitude and direction of the relationship between the variables.\n","\n","17. How does the intercept in a regression model provide context for the relationship between variables?\n","\n","ans- The intercept provides a baseline value for the dependent variable when all independent variables are zero. It can help understand the starting point or initial condition of the relationship.\n","\n","18. What are the limitations of using R² as a sole measure of model performance?\n","\n","ans- R² does not indicate if the coefficients are biased or if the model is correctly specified. It can also be inflated by adding more independent variables, even if they are not meaningful.\n","\n","19. How would you interpret a large standard error for a regression coefficient?\n","\n","ans- A large standard error indicates that the coefficient estimate is imprecise and may not be statistically significant. It suggests that the true coefficient value could vary widely.\n","\n","20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n","\n","ans- Heteroscedasticity can be identified in residual plots by observing a non-constant variance of residuals across predicted values. It's important to address it because it can lead to unreliable statistical inferences.\n","\n","21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n","\n","ans- It means that the model includes unnecessary independent variables that are not significantly contributing to the prediction of the dependent variable. Adjusted R² penalizes the inclusion of irrelevant variables.\n","\n","22. Why is it important to scale variables in Multiple Linear Regression?\n","\n","ans- Scaling variables can improve model performance by preventing variables with larger ranges from dominating the model. It can also help with convergence in gradient-based optimization algorithms.\n","\n","23. What is polynomial regression?\n","\n","ans- Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n","\n","24. How does polynomial regression differ from linear regression?\n","\n","ans- Linear regression models a linear relationship, while polynomial regression models a non-linear relationship using polynomial terms of the independent variable.\n","\n","25. When is polynomial regression used?\n","\n","ans- Polynomial regression is used when the relationship between the variables is non-linear and can be approximated by a polynomial function.\n","\n","26. What is the general equation for polynomial regression?\n","\n","ans- The general equation is: Y = b0 + b1X + b2X^2 + ... + bn*X^n, where n is the degree of the polynomial.\n","\n","27. Can polynomial regression be applied to multiple variables?\n","\n","ans- Yes, polynomial regression can be extended to multiple variables by including polynomial terms and interaction terms of the independent variables.\n","\n","28. What are the limitations of polynomial regression?\n","\n","ans- Polynomial regression can lead to overfitting, especially with high-degree polynomials. It can also be sensitive to outliers and require careful selection of the polynomial degree.\n","\n","29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n","\n","ans- Methods include:\n","Cross-validation.\n","Adjusted R².\n","AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n","Residual analysis.\n","\n","30. Why is visualization important in polynomial regression?\n","\n","ans- Visualization helps understand the shape of the fitted curve and identify potential issues like overfitting or poor fit in certain regions of the data.\n","\n","31. How is polynomial regression implemented in Python?\n","\n","ans- Polynomial regression can be implemented in Python using libraries like scikit-learn. The process involves:\n","Creating polynomial features using PolynomialFeatures.\n","Fitting a linear regression model to the polynomial features.\n","Making predictions using the fitted model."],"metadata":{"id":"hmfm4DmV_MlI"}}]}